{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Necessary Libraries**\n",
        "\n",
        "This imports Pandas (pd) for data manipulation and re (Regular Expressions) for pattern matching. Pandas helps with reading, cleaning, and processing data efficiently. The re module is used for tasks like validating email formats or standardizing text data."
      ],
      "metadata": {
        "id": "rE45iiE1BJJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "plkniddR-WO_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading the Dataset**\n",
        "The load_data(file_path) function is responsible for reading a CSV file and loading its contents into a Pandas DataFrame. This is done using pd.read_csv(file_path), which allows us to work with the data in a structured tabular format. The function returns the loaded DataFrame so that it can be processed further. If the file path is incorrect or the file is missing, an error will be raised."
      ],
      "metadata": {
        "id": "-p5XY3Wb_TGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"Loads customer data from a CSV file.\"\"\"\n",
        "    return pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "knM03fCx-YJl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Identifying Data Issues**\n",
        "The identify_issues(df) function helps in detecting common data quality problems. It first checks for missing values using df.isnull().sum(), which counts the number of NaN values in each column. Then, it identifies negative ages by counting how many values in the age column are less than zero. For invalid emails, the function uses a regular expression (regex) to filter out incorrectly formatted email addresses. Finally, it counts duplicate rows using df.duplicated().sum(), ensuring that we can remove redundant data later."
      ],
      "metadata": {
        "id": "ENc6p1Yj_dqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_issues(df):\n",
        "    \"\"\"Identifies missing values, invalid data, and duplicates.\"\"\"\n",
        "    issues = {\n",
        "        'missing_values': df.isnull().sum(),\n",
        "        'negative_ages': (df['age'] < 0).sum(),\n",
        "        'invalid_emails': df[~df['email'].str.match(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$', na=False)].shape[0],\n",
        "        'duplicates': df.duplicated().sum()\n",
        "    }\n",
        "    return issues"
      ],
      "metadata": {
        "id": "YUUIwmUO-cmp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Handling Missing Values**\n",
        "The handle_missing_values(df) function fills in missing data to ensure consistency in the dataset. First, it prints the column names for debugging purposes. Then, it replaces missing name values with \"Unknown\" so that every record has an identifiable name. If country is missing, it is filled with \"Not Specified\" to avoid empty fields. For purchase_amount, missing values are replaced with the column’s mean (average) to maintain numerical consistency. Instead of using inplace=True, the function assigns the modified DataFrame back to df, which helps prevent chained assignment warnings in Pandas."
      ],
      "metadata": {
        "id": "kLMH8Dc0_8hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_missing_values(df):\n",
        "    \"\"\"Fills missing values with appropriate defaults.\"\"\"\n",
        "    print(\"Column Names in DataFrame:\", df.columns)\n",
        "\n",
        "    df['name'] = df['name'].fillna(\"Unknown\")\n",
        "    df['country'] = df['country'].fillna(\"Not Specified\")\n",
        "    df['purchase_amount'] = df['purchase_amount'].fillna(df['purchase_amount'].mean())\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "mv-xjVls-fOh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fixing Invalid Data**\n",
        "The fix_invalid_data(df) function corrects incorrect entries. Some records might have negative ages, which do not make sense in a real-world scenario. This is fixed using df['age'].apply(lambda x: abs(x)), which converts negative numbers into positive values. Next, invalid emails are removed from the dataset using regex filtering, ensuring that only correctly formatted email addresses remain. This step improves the accuracy of email-based communication and prevents data entry errors.\n",
        "\n"
      ],
      "metadata": {
        "id": "FjHp6xsqADxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix invalid data\n",
        "def fix_invalid_data(df):\n",
        "    \"\"\"Fixes invalid data such as negative ages and invalid emails.\"\"\"\n",
        "    df['age'] = df['age'].apply(lambda x: abs(x))\n",
        "    df = df[df['email'].str.match(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$', na=False)]\n",
        "    return df"
      ],
      "metadata": {
        "id": "pxdpNptO-nHB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Standardizing Data**\n",
        "The standardize_data(df) function ensures data uniformity by cleaning phone numbers and formatting country names. The phone column is cleaned by removing non-numeric characters using df['phone'].astype(str).str.replace(r'[^0-9]', '', regex=True), which keeps only digits. This is useful for making phone numbers consistent across different formats. The country column is standardized by converting all country names to title case (e.g., \"india\" → \"India\") using df['country'].str.title(), improving readability and consistency.\n",
        "\n"
      ],
      "metadata": {
        "id": "BLsjYQqAAL1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_data(df):\n",
        "    \"\"\"Standardizes phone numbers and country names.\"\"\"\n",
        "    df['phone'] = df['phone'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n",
        "    df['country'] = df['country'].str.title()\n",
        "    return df"
      ],
      "metadata": {
        "id": "S7-V0B5m-py2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Removing Duplicates**\n",
        "The remove_duplicates(df) function eliminates duplicate rows from the dataset using df.drop_duplicates(). This is important because duplicate records can lead to incorrect analysis, especially when counting unique customers or calculating statistics. By removing duplicates, the dataset becomes cleaner and more reliable."
      ],
      "metadata": {
        "id": "pAdbgwimAX_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicates(df):\n",
        "    \"\"\"Removes duplicate records.\"\"\"\n",
        "    return df.drop_duplicates()"
      ],
      "metadata": {
        "id": "kHV92h9x-sng"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Detecting Outliers in Purchase Amount**\n",
        "The detect_outliers(df) function identifies unusual purchase amounts using the interquartile range (IQR) method. First, it calculates the first quartile (Q1) and third quartile (Q3), then determines the IQR (Q3 - Q1). Any values that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR are considered outliers. Detecting outliers is crucial for identifying potential errors, fraud, or extreme spending behaviors."
      ],
      "metadata": {
        "id": "F1MCwoczApTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_outliers(df):\n",
        "    \"\"\"Detects outliers in purchase_amount using the IQR method.\"\"\"\n",
        "    q1, q3 = df['purchase_amount'].quantile([0.25, 0.75])\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    return df[(df['purchase_amount'] < lower_bound) | (df['purchase_amount'] > upper_bound)]"
      ],
      "metadata": {
        "id": "1QePJj8v-u5m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saving Cleaned Data**\n",
        "The save_cleaned_data(df, output_file) function saves the cleaned DataFrame to a new CSV file using df.to_csv(output_file, index=False). By setting index=False, it ensures that the DataFrame’s index is not included in the output file. This allows the cleaned dataset to be used for further analysis or machine learning tasks without unnecessary columns."
      ],
      "metadata": {
        "id": "bLVw8-3sAulV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_cleaned_data(df, output_file):\n",
        "    \"\"\"Saves the cleaned dataset to a new CSV file.\"\"\"\n",
        "    df.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "jVoX2yT7-yMN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Running the Cleaning Process**\n",
        "The if __name__ == \"__main__\": block runs the full cleaning pipeline step by step. It first loads the dataset, identifies issues, and prints them before cleaning. The dataset then goes through missing value handling, invalid data correction, standardization, and duplicate removal. After cleaning, issues are checked again to verify improvements. Finally, the cleaned dataset is saved, and a confirmation message is displayed.\n",
        "\n",
        "By following this structured approach, the script ensures the dataset is clean, standardized, and free of errors, making it suitable for further analysis or machine learning applications."
      ],
      "metadata": {
        "id": "RVhCVsl_A0-f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNh7Zakb-K8I",
        "outputId": "c520824b-38c5-423f-b024-93639f3ca6f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Issues before cleaning:\n",
            "{'missing_values': customer_id         0\n",
            "name                5\n",
            "email               0\n",
            "phone              20\n",
            "age                 0\n",
            "country             8\n",
            "purchase_amount    15\n",
            "dtype: int64, 'negative_ages': 0, 'invalid_emails': 19, 'duplicates': 0}\n",
            "Column Names in DataFrame: Index(['customer_id', 'name', 'email', 'phone', 'age', 'country',\n",
            "       'purchase_amount'],\n",
            "      dtype='object')\n",
            "Issues after cleaning:\n",
            "{'missing_values': customer_id        0\n",
            "name               0\n",
            "email              0\n",
            "phone              0\n",
            "age                0\n",
            "country            0\n",
            "purchase_amount    0\n",
            "dtype: int64, 'negative_ages': 0, 'invalid_emails': 0, 'duplicates': 0}\n",
            "Cleaned data saved to /content/customers_cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/customers.csv\"\n",
        "    output_file = \"/content/customers_cleaned.csv\"\n",
        "\n",
        "    df = load_data(file_path)\n",
        "\n",
        "    print(\"Issues before cleaning:\")\n",
        "    print(identify_issues(df))\n",
        "\n",
        "    df = handle_missing_values(df)\n",
        "    df = fix_invalid_data(df)\n",
        "    df = standardize_data(df)\n",
        "    df = remove_duplicates(df)\n",
        "\n",
        "    print(\"Issues after cleaning:\")\n",
        "    print(identify_issues(df))\n",
        "\n",
        "    save_cleaned_data(df, output_file)\n",
        "    print(f\"Cleaned data saved to {output_file}\")\n",
        "\n"
      ]
    }
  ]
}